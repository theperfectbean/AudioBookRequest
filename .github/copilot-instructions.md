# Copilot Instructions for AudioBookRequest

AudioBookRequest is a FastAPI-based audiobook request management system for Plex/Audiobookshelf/Jellyfin servers with Prowlarr integration for auto-downloading.

## Architecture Overview

**Core Stack:** FastAPI + SQLModel (SQLAlchemy + Pydantic) + Jinja2 + HTMX + Tailwind CSS + Alembic migrations

**Request Flow:**
1. HTTP request → DynamicSessionMiddleware (auth + session setup)
2. Router handler → injects `Session` (DB) and `ClientSession` (HTTP) via FastAPI Depends
3. Handler calls business logic in `app/internal/` → queries DB or external APIs
4. Response: HTML template or JSON

**Key Models:** User (auth with GroupEnum), Audiobook (cached metadata), AudiobookRequest (user requests), APIKey (programmatic access)

**Database:** SQLite (default) or PostgreSQL via `ABR_DB__USE_POSTGRES`. Migrations in `alembic/versions/`

## Directory Structure

- `app/main.py` - App init, middleware, exception handlers
- `app/routers/` - Web page routes (search, wishlist, settings, auth)
- `app/routers/api/` - REST API endpoints (under `/api`)
- `app/internal/` - Core business logic:
  - `auth/` - Authentication (forms, OIDC, basic, API keys, sessions)
  - `models.py` - SQLModel definitions
  - `db_queries.py` - Common SQL queries
  - `prowlarr/` - Prowlarr integration for indexer searches
  - `metadata/` - Book metadata enrichment (Google Books)
  - `ranking/` - Download source quality scoring
  - `indexers/` - Custom indexer implementations
- `app/util/db.py` - Database engine/session setup
- `app/util/log.py` - Structured logging with structlog
- `templates/` - Jinja2 HTML templates

## Development Workflow

**Setup & Running:**
```bash
uv sync                    # Install dependencies (uses uv, not pip)
just dev                   # Runs migrations then `fastapi dev`
just migrate               # Run pending migrations
just create_revision "msg" # Create autogenerated migration
just tailwind              # Watch CSS compilation
just types                 # Lint + type check + format check
uv run pytest tests/       # Run tests
```

**Claude Code Implementation Plans:**
- Implementation plans stored in `.claude-plans/` (symlinked to `~/.claude/plans`)
- Plans follow naming convention: `{feature-name}.md` or `{task-type}/{task-name}.md`
- Useful for tracking multi-step refactorings, complex features, or integration work
- Claude Code can reference these for context on in-progress tasks

### Plan Management Workflow

**Workflow Division:**
- **Claude Code (Opus)**: Handles PLANNING - architectural decisions, feature design, implementation strategy. Creates plans in `.claude-plans/`.
- **Copilot**: Handles IMPLEMENTATION - executes planned features, adds endpoints, creates new files, multi-file refactoring. Follows plans and documents execution.

**Plan Naming Best Practices:**
```
feature-{name}.md           # New features (e.g., feature-openlib-metadata.md)
bugfix-{issue}.md           # Bug fixes (e.g., bugfix-prowlarr-timeout.md)
refactor-{component}.md     # Refactoring work (e.g., refactor-metadata-providers.md)
```

**Before Implementing (Copilot):**
1. **Always check `.claude-plans/` for existing plans** related to your work
   - `grep -r "metadata provider\|ranking\|auth" .claude-plans/`
   - If a plan exists, follow it closely unless technical constraints require deviations
   - If no plan exists for a major architectural change, stop and request Claude Code create one
2. Read existing implementation patterns in referenced files (e.g., study `GoogleBooksProvider` before adding new metadata provider)
3. Check CLAUDE.md for any recent context from Claude Code sessions

**What Copilot Can Implement (With a Plan):**
- New features following architectural design
- API endpoints and route handlers
- Database queries and migrations
- New files and multi-file refactoring
- Bug fixes for specified issues
- **Constraint**: Avoid major architectural changes without an explicit plan from Claude Code

**After Implementing (Copilot):**
1. Update the plan file with:
   - `Status: Complete` and implementation date
   - **Files Changed**: List all modified/created files with brief descriptions
   - **Deviations**: Document any differences from original plan with reasoning
   - Example:
     ```markdown
     ## Status: Complete (2026-01-17)
     
     ### Files Changed
     - `app/internal/metadata/openlib.py` - New provider implementation (200 lines)
     - `app/internal/metadata/__init__.py` - Added import/export
     - `tests/test_openlib.py` - Added unit tests
     
     ### Deviations from Plan
     - Used sync HTTP instead of async (cached OpenLibrary API slower with aiohttp)
     - Cache key logic simplified: only hash title+author, not ISBN
     ```

2. Update CLAUDE.md with implementation record:
   - **What was built**: Concise 1-2 sentence description
   - **Plan followed**: Reference the plan filename: `(see feature-openlib-metadata.md)`
   - **Deviations**: Any necessary changes from the plan with rationale
   - **Lessons learned**: New patterns discovered or edge cases encountered
   - Example:
     ```markdown
     ## 2026-01-17: OpenLibrary Metadata Provider
     
     Implemented new metadata enrichment provider following feature-openlib-metadata.md. 
     Used sync HTTP client instead of async (plan rationale: cached API responses reduce 
     concurrency benefit). Discovered that fuzzy matching on title alone (without ISBN) 
     causes 15% false positives; updated to require author match too.
     ```

3. Use descriptive commit messages that reference the plan:
   ```
   feat: add OpenLibrary metadata provider (see feature-openlib-metadata.md)
   
   - Implements MetadataProvider interface for Google Books fallback
   - Added 30-day cache via MetadataCache table
   - Includes fuzzy matching for title/author verification
   
   Deviations: Used sync HTTP client for better cache performance
   ```

4. Document complex logic inline:
   - Add comments explaining "why" decisions, not just "what" code does
   - Example: `# Use token_sort_ratio (not simple ratio) because book titles vary in word order`
   - This helps Claude Code understand architectural intent in future sessions

**Maintaining Cross-Session Context:**
- CLAUDE.md is the "implementation journal" - Claude Code checks this on startup to understand what Copilot has accomplished
- Plans in `.claude-plans/` are the "work registry" - reference them in CLAUDE.md updates
- Commit messages bridge the two - they document what was changed and why
- This creates a discoverable chain: Claude Code → CLAUDE.md → .claude-plans/ → Code changes → Inline comments
- **Result**: Claude Code can pick up context from Copilot's work, and Copilot can execute Claude Code's plans without back-and-forth

**Key Tools:**
- `uv` for dependency management (replaces pip/poetry)
- `Alembic` for database migrations (SQLAlchemy-based)
- `basedpyright` for type checking (Python 3.12+ required)
- `djlint` for template linting (Jinja profile)
- `ruff` for formatting/linting
- `pytest` for testing with `pytest-asyncio` for async tests

## Critical Patterns & Conventions

### Dependency Injection
FastAPI Depends pattern used throughout. Common injections:
```python
from typing import Annotated
from fastapi import Depends
from app.util.db import get_session
from app.util.connection import get_connection
from app.internal.auth.authentication import ABRAuth, DetailedUser

session: Annotated[Session, Depends(get_session)]           # DB session
client_session: Annotated[ClientSession, Depends(get_connection)]  # HTTP
user: Annotated[DetailedUser, Security(ABRAuth())]         # Current user + login type
```

### Authentication
Multiple auth methods configured via `ABR_APP__FORCE_LOGIN_TYPE` (forms/oidc/basic/api_key/none). ABRAuth dependency handles all types. User groups: untrusted (manual approval) → trusted (auto-download) → admin (full access).

### Configuration
Environment variables prefixed with `ABR_` using double underscore nesting:
- `ABR_APP__DEBUG`, `ABR_APP__PORT`, `ABR_APP__CONFIG_DIR`
- `ABR_DB__USE_POSTGRES`, `ABR_DB__POSTGRES_*`
Load from `.env.local` or `.env` via pydantic-settings in [env_settings.py](app/internal/env_settings.py)

### Logging
Structured logging with **structlog** - all loggers from `app.util.log.logger`. Use `logger.info("event", key=value)` for structured output (no f-strings in messages).

### Database
- **Session Management:** Get fresh session via `Depends(get_session)` in route handlers. For background tasks/scripts: `with next(get_session()) as session: ...`
- **Queries:** Use SQLModel's select() + session.exec(). Common queries in [db_queries.py](app/internal/db_queries.py)
- **Migrations:** Autogenerate with `just create_revision "msg"`. Manual edits rarely needed except PostgreSQL ALTER TABLE unique constraints

### API Endpoints
All routes are `async`. Request bodies use Pydantic models for validation. Response can be template (Jinja2) via `template_response()` or JSON via `JSONResponse` / raw dict (FastAPI auto-converts).

### Template Response Pattern
```python
from app.util.templates import template_response

return template_response(
    request, "template.html",
    {"context_key": value},  # Template variables
    status_code=200
)
```

### Background Tasks & Scheduled Jobs
Use FastAPI's `BackgroundTasks` parameter in route handlers. Long-running tasks (notifications) use APScheduler for scheduling.

## Integration Points

- **Prowlarr:** Query via HTTP for torrent sources. Virtual ASINs for non-Audible books. Config stored in DB
- **Audible API:** Book search & metadata via [book_search.py](app/internal/book_search.py)
- **Google Books:** Fallback metadata enrichment
- **Notifications:** Via Apprise library (gotify, Discord, etc.)
- **External Auth:** OIDC federation configurable via settings

## Docker Deployment

### Container Architecture
**Multi-stage build** (Dockerfile):
1. CSS stage - Compiles Tailwind CSS with DaisyUI modules
2. Python deps stage - Installs dependencies via `uv` into `.venv`
3. Final stage - Alpine Python 3.14 with compiled CSS, vendored JS, migrations, app code

**Static File Handling:**
- CSS compiled to `static/globals.css` (done at build time)
- JS modules fetched by `app/util/fetch_js.py` during deps stage
- All static files copied to final image (cannot be dynamically modified in containers)
- Template static references use relative paths; ensure base URL configured correctly

### Docker Compose Networking
- `web` service (AudioBookRequest) on bridge network `default`
- For Prowlarr integration: Use service name `prowlarr` or `host.docker.internal:host-gateway` for host access
- PostgreSQL service example: `ABR_DB__POSTGRES_HOST=psql` (service name, not localhost)
- Gotify notifications: `http://gotify:80/message?token=...` (service-to-service)

### Common Debugging Scenarios

**Missing Static Files:**
- Rebuild image: `docker compose --profile local build --no-cache`
- Verify `static/` copied in Dockerfile
- Check browser network tab for 404s on `/static/*` paths
- Clear browser cache (Shift+Ctrl+R)

**Prowlarr Connection Issues:**
- If Prowlarr in separate `docker-compose.yml`, ensure networks match
- Test connectivity from web container: `docker exec <web-container> curl http://prowlarr:8080/...`
- Config stored in DB - verify `ABR_PROWLARR__BASE_URL` and `API_KEY` settings via admin UI
- Check Prowlarr logs for blocked/invalid requests

**Database Migrations in Containers:**
- Migrations run automatically on startup (set by `just dev` locally)
- In production, ensure migration runs before app startup
- For PostgreSQL: Run `alembic upgrade heads` before starting app (can add to Docker entrypoint)
- Check migration logs: `docker logs <container> | grep -i "alembic\|migration"`

### Local Development Docker Profile
```bash
# Build and start with local profile (includes gotify, postgres, web)
docker compose --profile local up --build

# Access: http://localhost:8000, Gotify: http://localhost:8082
# Config/data persisted in ./config and ./data directories
```

## Metadata Enrichment Patterns

### Architecture Overview
Metadata enrichment provides fallback book details for **virtual books** (books from indexers not in Audible). The flow:
1. User searches → Prowlarr finds book → no Audible ASIN found
2. **Generate virtual ASIN** (deterministic hash of title+author)
3. Create `Audiobook` record with virtual ASIN
4. **Optional**: Enrich with Google Books metadata (cover, description, rating)
5. Cache result for 30 days to avoid repeated API calls

### Virtual ASIN Generation
```python
# Format: VIRTUAL-{11-char-hex-hash}
# Example: VIRTUAL-a3f9b8d2c1 for "Evolution of God" by "Robert Wright"

def generate_virtual_asin(title: str, author: str) -> str:
    """Deterministic - same title+author always generates same ASIN."""
    norm_title = normalize_text(title, primary_only=True)[:50]
    norm_author = normalize_text(author)[:30]
    hash_input = f"{norm_title}:{norm_author}"
    stable_hash = hashlib.sha256(hash_input.encode()).hexdigest()[:11]
    return f"VIRTUAL-{stable_hash}"

# Used in: app/routers/api/search.py:58
```

### Google Books Provider Implementation

**Metadata Model** (`app/internal/models.py:305`):
```python
class MetadataCache(BaseSQLModel, table=True):
    search_key: str = Field(primary_key=True)         # SHA256 hash of title:author
    provider: str = Field(primary_key=True)          # "google_books"
    metadata_json: str                                # Serialized EnrichedMetadata
    created_at: datetime                              # Auto-set for 30-day expiry check
```

**Cache Strategy:**
- `search_key` = SHA256(title:author)[:16] for deterministic lookups
- 30-day TTL (configured via `ABR_APP__METADATA_CACHE_EXPIRY_DAYS`, default 30)
- Expired entries deleted on next lookup
- Use `session.exec(select(MetadataCache).where(...)).first()` to check cache

**Enrichment Process** (`app/internal/metadata/google_books.py`):
1. **Normalize book title/author** to avoid fuzzy duplicates
2. **Check cache** with search_key - if valid return, if expired delete
3. **Query Google Books API** with author+title search
4. **Parse response** and extract best metadata (prefer ISBN_13, large cover images)
5. **Fuzzy match** top result against original (using `rapidfuzz`) to ensure relevance
6. **Store to cache** as JSON blob for future lookups

**Fuzzy Matching with rapidfuzz:**
```python
from rapidfuzz import fuzz, utils

# Settings.app.fuzzy_match_cache_ttl (default: 3600 seconds)
# Caching layer in app/internal/prowlarr/util.py:83-124

def cached_fuzzy_ratio(text1: str, text2: str, algo: str = "token_sort_ratio") -> float:
    """Cached wrapper for rapidfuzz with TTL management."""
    # token_sort_ratio normalizes word order, ideal for titles
    # Threshold typically 80+ for book matching
    score = fuzz.token_sort_ratio(utils.default_process(text1), 
                                  utils.default_process(text2))
    return score
```

**Test Case Reference** - "Evolution of God by Robert Wright":
- Virtual ASIN generated: `VIRTUAL-{hash}`
- Search key: SHA256("evolution of god:robert wright")[:16]
- Google Books typically returns: ISBN-13, description, cover image, rating
- Fuzzy match score should exceed `ABR_APP__AUTHOR_MATCH_THRESHOLD` (default 70)

### Adding New Metadata Providers

To add (e.g., OpenLibrary provider):
1. **Create provider file**: `app/internal/metadata/openlib.py`
2. **Define response models** (Pydantic for API contracts)
3. **Implement class** with async methods:
   ```python
   class OpenLibraryProvider:
       async def check_cache(self, session: Session, search_key: str) -> Optional[EnrichedMetadata]
       async def enrich_virtual_book(self, session: Session, book: Audiobook) -> Audiobook
   ```
4. **Use existing cache table** - just add new `provider` value to `MetadataCache.provider`
5. **Export in `__init__.py`**: `from .openlib import OpenLibraryProvider`
6. **Call in search flow** (example in `app/routers/api/search.py`)

## Common Query Patterns

### Basic Lookups by Primary Key
```python
from sqlmodel import Session, select
from app.internal.models import Audiobook, User

# Single book by ASIN
book = session.exec(select(Audiobook).where(Audiobook.asin == "B002V00TOO")).first()

# Single user by username
user = session.exec(select(User).where(User.username == "alice")).first()

# Exists check
exists = session.exec(select(Audiobook).where(Audiobook.asin == asin)).first() is not None
```

### Filtering with Multiple Conditions
```python
from sqlmodel import col, not_

# Books requested by specific user, not yet downloaded
results = session.exec(
    select(Audiobook)
    .join(AudiobookRequest)
    .where(
        AudiobookRequest.user_username == "alice",
        not_(Audiobook.downloaded)
    )
).all()

# All admins
admins = session.exec(select(User).where(User.group == GroupEnum.admin)).all()
```

### Relationships & Joins
```python
from sqlalchemy.orm import selectinload
from app.internal.models import AudiobookWishlistResult

# Fetch audiobook with all related requests (eager load)
results = session.exec(
    select(Audiobook)
    .options(selectinload(Audiobook.requests))
    .where(col(Audiobook.asin).in_(asin_list))
).all()

# Iterate relationships without N+1 queries
for book in results:
    for request in book.requests:
        print(request.user_username)
```

### Aggregations & Counting
```python
from sqlalchemy import func

# Total books count
total = session.exec(select(func.count()).select_from(Audiobook)).one()

# Count downloads by status
counts = session.exec(
    select(Audiobook.downloaded, func.count("*"))
    .select_from(Audiobook)
    .join(AudiobookRequest)
    .group_by(col(Audiobook.downloaded))
).all()

for downloaded_status, count in counts:
    print(f"Downloaded: {downloaded_status}, Count: {count}")
```

### Cache Queries with Expiry Checks
```python
from datetime import datetime, timedelta

# Check metadata cache (with auto-expiry)
cache_entry = session.exec(
    select(MetadataCache)
    .where(
        MetadataCache.search_key == search_key,
        MetadataCache.provider == "google_books"
    )
).first()

if cache_entry:
    age_days = (datetime.now() - cache_entry.created_at).days
    if age_days > 30:
        session.delete(cache_entry)
        session.commit()
        return None  # Treat as cache miss
```

### Batch Operations
```python
# Create many records in transaction
try:
    for item in batch:
        session.add(item)
    session.commit()
except SQLAlchemyError as e:
    session.rollback()
    logger.error("Batch insert failed", error=str(e))

# Update multiple records
session.exec(
    update(Audiobook)
    .where(col(Audiobook.asin).in_(asin_list))
    .values(downloaded=True)
)
session.commit()
```

### Session Usage Patterns

**Route Handlers (sync injection):**
```python
from typing import Annotated
from fastapi import Depends

async def my_handler(
    session: Annotated[Session, Depends(get_session)]
) -> dict:
    book = session.exec(select(Audiobook).where(...)).first()
    return {"asin": book.asin}
```

**Background Tasks/Scripts (manual context):**
```python
from app.util.db import get_session

with next(get_session()) as session:
    books = session.exec(select(Audiobook)).all()
    for book in books:
        process(book)
```

**Async Handlers (same pattern, different context):**
```python
@router.get("/items")
async def list_items(session: Annotated[Session, Depends(get_session)]):
    # Session is sync; queries run in event loop
    items = session.exec(select(Item)).all()
    return items
```

## Testing Notes

- Tests in `tests/` with in-memory SQLite fixture (`db_engine`, `db_session`)
- Use `pytest-asyncio` for async handlers
- **IMPORTANT:** Do NOT run test suites automatically - only suggest commands for manual execution
- Focus on code generation, not test validation

## Important Details

- **Python 3.12+ required** (uses new generics syntax in models)
- Conventional Commits for commit messages
- CSRF protection via session middleware
- SQLite: Foreign keys pragma auto-enabled in get_session()
- Root user in OIDC: Any user with root admin username becomes root admin regardless of OIDC groups
